{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diário de Desenvolvimento - CSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dia 1 - (26 de agosto de 2024):\n",
    "\n",
    "    Iniciamos a reunião com o Professor Cledson, onde ele explicou sobre o projeto. Depois ouvimos as contribuíções dos outros autores até então, e ficamos responsáveis por entender o ambiente e replicá-lo com as devidas adaptações e melhorias.\n",
    "\n",
    "    Conclusão da reunião: \n",
    "    \n",
    "        O objetivo desse Projeto/Artigo é:\n",
    "\n",
    "                - Descrever a evolução do desenvolvimeto do Artigo/Projeto sobre Human Activity Recognition (HAR), ou melhor, Reconhecimento de Atividade Humana, utilizando sinais de Redes Wi-Fi.\n",
    "\n",
    "                - Cirar a base da seção 5 do artigo que tratará sobre:\n",
    "                \n",
    "                        + Conjunto de Dados: Neste estudo, utilizaremos o conjunto de dados eHealth Channel State Information (CSI), que é um recurso amplamente reconhecido para o reconhecimento de atividades humanas (HAR). Este conjunto de dados engloba informações detalhadas de 56 indivíduos, proporcionando uma amostra diversificada e representativa. A coleta de dados foi realizada sob rigorosas condições controladas, e detalhes específicos sobre essa coleta podem ser encontrados em estudos anteriores, devidamente referenciados.               \n",
    "                    \n",
    "                        + Sanitização de Dados: A sanitização e normalização dos dados são etapas cruciais para garantir a precisão e a robustez das análises subsequentes. Neste trabalho, enfatizaremos a importância de procedimentos rigorosos de limpeza de dados, que incluem a normalização dos dados de amplitude, o tratamento de valores inválidos e a exclusão de rótulos incorretos. Esses processos são essenciais para minimizar ruídos e variações indesejadas, assegurando que as conclusões obtidas sejam confiáveis e reproduzíveis.\n",
    "\n",
    "                        + Análise Inicial: Propomos que a utilização de todos os subportadores para a tarefa de HAR pode ser não apenas redundante, mas também contraproducente, criando conflitos que impactam negativamente o desempenho dos modelos de aprendizado de máquina. Observações consistentes em todas as posições, indivíduos e subportadores, com variações mínimas, indicam que a inclusão irrestrita de todos os subportadores pode adicionar ruído desnecessário ao modelo, comprometendo sua eficácia.\n",
    "\n",
    "                        + Hipótese: Propomos que a utilização de todos os subportadores para a tarefa de HAR pode ser não apenas redundante, mas também contraproducente, criando conflitos que impactam negativamente o desempenho dos modelos de aprendizado de máquina. Observações consistentes em todas as posições, indivíduos e subportadores, com variações mínimas, indicam que a inclusão irrestrita de todos os subportadores pode adicionar ruído desnecessário ao modelo, comprometendo sua eficácia.\n",
    "\n",
    "                        Metas: As metas desta seção são: \n",
    "                            1) Testar a hipótese de que a utilização seletiva de subportadores pode otimizar o desempenho dos modelos de aprendizado de máquina em HAR; \n",
    "                            2) E, propor uma solução prática e avaliar se essa abordagem resulta em uma melhoria mensurável na precisão e eficiência dos modelos.\n",
    "\n",
    "\n",
    "        Autores: Emanuel Coimbra  e Lucas Huguenin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encerrada reunião, entramos novamente em conferência, agora apenas nós dois para organizar o ambiente, e tomar decisões iniciais de projeto que foram:\n",
    "    \n",
    "    1) Criar um repositório paralelo para a não alterar o que já foi construído pelo Professor Cledson:\n",
    "    2) Criar esse diário para registrar dúvidas e avanços de implementação.\n",
    "    3) Criar uma estrutura de notebooks para cada etapa do processo de forma individual: \n",
    "        - Extração de dados brutos (PCAPs); \n",
    "        - Tratamento de dados (serialização dos dados de maneira a preservar a informação da amplitude de cada portadora, garantindo que os dataframes individuais sejam gerenciados de forma a manter tudo em um único arquivo sempre que possível); \n",
    "        - Armazenamento e disponibilização (utilização de formatos padrão como Pickle ou Protocol Buffers para facilitar o trabalho com streams); Treinamento dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dia 2 - (30 de setembro de 2024):\n",
    "\n",
    "    Discutimos o modelo de aquisição e compilação dos dados para tornar mais factível sua implementação como input de dados para treinamento de diferentes modelos. E percebemos pontos de atenção:\n",
    "\n",
    "        - Trabalhar com arquivos binários para reduzir tamanhos.\n",
    "\n",
    "        - Trabalhar com streams para reduzir uso de memória em RAM.\n",
    "\n",
    "        - Alterar input do modelo para agregar todas as subportadoas em uma passagem.\n",
    "\n",
    "        - Utilizar tipo complex de python para tratar os dados ao invés da função de conversão.\n",
    "\n",
    "        - Remover conflito de nome em métodos print com o método padrão de python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dia 3 - (2 de setembro de 2024): \n",
    "\n",
    "    1) Atualizamos a forma de extração de dados, substituindo a execução monolítica por uma paralelizada, com o objetivo de economizar tempo na leitura dos PCAPs.\n",
    "\n",
    "    Monolítico:\n",
    "        Tempo obtido para 1 pessoa: 65,8 segundos\n",
    "        Tempo estimado para 56 pessoas: 61,6 minutos\n",
    "    \n",
    "    Tamanho gerado total do arquivo esperado: 10 gb\n",
    "\n",
    "    Paralelizado em  Pool:\n",
    "        Tempo obtido para 1 pessoa: 15,09 segundos\n",
    "        Tempo esperado  para 56 pessoa: 17,61 minutos\n",
    "        Tempo obtido para 56 pessoa: 768,7 segundos (12,8 minutos)\n",
    "\n",
    "    Tamanho gerado: 7GB\n",
    "    \n",
    "    2) Transformamos ele em Pickle, formato padrão o qual utilizaremos a partir daqui.\n",
    "\n",
    "    3) Adotamos como forma de importação padrão o stream pra gerenciar de maneira mais racionaçl os recursos de mémoria RAM.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
